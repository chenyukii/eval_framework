**概览与代码流**

- 主入口 eval_main.py: 解析 CLI 与可选 JSON/YAML 配置，归一化任务名（normalize_task_name），校验是否在 metrics/__init__.py 的 SUPPORTED_TASKS 内，然后分为单文件模式或目录模式；目录模式下用同一份指标对象流式累计，避免内存爆炸。
- 文件/目录评估封装 file_evaluator.py: FileEvaluationRunner 处理单文件对，DirEvaluationRunner 处理目录（可递归），内部依赖 DataLoader 读/验/配对，再交给 EvaluationCore 做解析+指标。
- 评估核心 evaluator.py: 提供一次性（evaluate_raw_paired_samples）与流式（update_metrics_with_raw_pairs）接口。核心步骤：按任务构建 MetricCollection → DataParser 解析 gt/输出 → metrics.update → metrics.compute_all。
- 数据处理 data/loader.py: 读取标注/输出文件（行级 JSON、JSON 数组、单 JSON 对象），批量读取目录；pair_samples 先按 source，再按 prompt 辅助；提供“加载+验证”封装，日志路径可配。
- 数据解析与校验 data/parser.py/data/validator.py: Parser 按任务把字符串解析成结构化（列表、数字、框、(count,list) 等）；Validator 检查必填字段、任务是否受支持（is_supported_task）、解析是否成功，记录错误/无效日志。
- 指标体系 metrics/*: MetricCollection 统一 update/compute_all。子模块：classification.py（Accuracy、Macro-F1、Macro-Recall、绝对误差/MAE）、retrieval.py（Accuracy/Recall/F1）、detection.py（AP@IoU=阈值，支持水平/旋转框）、caption.py（CIDEr/ROUGE-L/BLEU-4/METEOR，依赖 pycocoevalcap）。工具 utils/iou.py、utils/text_metrics.py 提供 IoU 与官方文本指标封装。日志工具在 utils/logger.py。
- 配置示例 config/cfg_*.json: 为各任务给出目录、是否递归、是否算辅助指标、输出路径等；task_metric.json 给出任务清单与部分指标映射（参考用），field_mapping.json/vis_config.json 为空占位。

**数据格式与配对**

- 标注必填字段：prompt, frames, gt, task, source。模型输出必填：sample_id, task, model_output, source。
- Loader 读文件：首字符为 [/{ 时整文件 json.load，否则逐行 json.loads。
- 校验失败/解析失败会写入 error_log.txt 或 invalid_sample_log.txt（路径可在 DataLoader/Validator 初始化时传入）。
- 配对策略：优先 source 精确匹配（支持同一 source 多条时逐个消费）；失败再用 prompt 精确字符串匹配；记录未配对数并告警。

**任务与指标逻辑**

- 任务归一化：metrics/__init__.py 的 TASK_ALIASES（如 quyujianceHBB → 水平区域检测）。is_supported_task 也走归一化。

- 分类/VQA/计数 (metrics/classification.py):

  - Accuracy：标签集合完全相等才算对，空集合对空集合会被跳过不计数。
  - Macro-Recall/Macro-F1：按类累计 TP/FP/FN；只对 gt 中出现的类计召回；precision/recall 为 0 时 F1 置 0。
  - 绝对误差：VQA2 记为 abs_error，计数记为 mae，为辅助指标。

- 检测/定位/VQA3 (metrics/detection.py):

  - 输入格式：水平/旋转检测期望 (count, box_list)，VQA3/视觉定位期望 box_list。
  - IoU：水平框用 bbox_iou，旋转框用 quad_iou（Sutherland-Hodgman 裁剪求交）。面积≤0 直接返回 0。
  - AP：目前无置信度，所有 score=1，仅一个 PR 点，AP 退化为该点面积；gt/pred 为空或正样本数为 0 返回 0。
  - IoU 阈值：0.5 为核心，0.75（检测/VQA3 辅助）、0.25（视觉定位辅助）。

- 检索 (metrics/retrieval.py):

  - Accuracy：预测集合与 gt 集合完全一致。
  - Recall：命中 gt 中目标的比例。
  - F1：全局 TP/FP/FN 微平均。
  - 空集合对空集合样本会被跳过。

- 描述 (

  metrics/caption.py+utils/text_metrics.py):

  - 核心：CIDEr、ROUGE-L；辅助：BLEU-4、METEOR。使用 pycocoevalcap 官方实现，需安装依赖并保证 Java 环境（Meteor）。
  - corpus 级计算：累积所有 (gt,pred) 后统一 compute。

- 特殊/异常处理：

  - Parser 返回 None 视为格式错误；Validator 记录日志并标无效。
  - Detection: 无 gt 或无 pred 时 AP 会变 0（npos=0 或无预测）。这意味着“全无目标”场景被算作 0，需要外部明确是否接受。
  - 绝对误差/MAE：非数字会被静默跳过。
  - caption 指标依赖包缺失时 _ensure_coco_evalcap 抛错，需提前安装。

**运行方式**

- 单文件模式：python eval_main.py --task VQA2 --annotation_file <标注.txt> --model_output_file <输出.txt> --no_aux_metric。路径也可在配置 JSON/YAML 中给出，用 --config 传入。
- 目录模式：python eval_main.py --config config/cfg_count.json 或命令行指定 --task 计数 --annotation_dir <anno_dir> --model_output_dir <out_dir> --pattern "*.txt" --recursive。
- 结果输出：终端打印 metrics（保留 2 位小数）与 stats；若提供 --output_json 会写出 {metrics, stats}。
- 快速复用代码：若需在 Python 内调用，可直接使用 file_evaluator.run_evaluation_for_file 或 run_evaluation_for_dir；需要更灵活流式时使用 EvaluationCore.build_metrics + update_metrics_with_raw_pairs。

**扩展与复用指南**

- 新任务接入（图像级/区域级）：
  1. 在 metrics/__init__.py 的 TASK_* 集合和 SUPPORTED_TASKS 中登记任务名（必要时在 TASK_ALIASES 增加别名）。
  2. 在 data/parser.py 的 _parse_by_task 增加解析逻辑（返回统一结构，便于指标模块使用）。
  3. 在 data/validator.py 的 supported_tasks 集合同步新增，必要时增加字段检查。
  4. 在对应指标模块实现 build_xxx_metrics 分支，或新建指标文件并在 build_task_metrics 调用。
  5. 若目录/文件评估要用，确保输出/标注格式匹配当前解析格式，必要时扩展配对规则。
- 像素级任务扩展（语义分割/变化检测）推荐步骤：
  1. 定义输出/gt 规范：例如 RLE/PNG 路径或多边形列表；在 Parser 中新增解析（可返回二值 mask、RLE 字符串或 np.ndarray 引用），尽量避免将大 mask 展开在内存，可设计为路径 + 延迟加载。
  2. Validator：增加对 mask/RLE 的格式检查与空值判断，必要时跳过大文件的 base64 校验（符合现有“跳过 base64”要求）。
  3. Metrics：新建 metrics/segmentation.py（如 mIoU、Dice），实现 BaseMetric 子类，支持流式累计（按类别混淆矩阵或像素计数），避免一次性读全量；在 build_task_metrics 添加分支。
  4. 评估流程：仍复用 EvaluationCore；在 Detection 类似的 update 里把 source 当作 image_id；注意目录评估时 DirEvaluationRunner 的配对逻辑是否需要按文件名而非 source。
- 新指标编写：
  - 继承 BaseMetric：实现 reset(初始化状态)、update(接收 gt/pred/task/source)、compute(返回原始数值)；数值在 compute_all 会统一保留两位小数。
  - 将新指标加入对应 build_xxx_metrics 并设置 is_aux 以便 CLI 开关。
  - 若指标依赖外部库，在 utils 做延迟导入 + 友好报错（参考 _ensure_coco_evalcap）。
- 复用已有代码的注意点：
  - 流式评估：使用 EvaluationCore.build_metrics + update_metrics_with_raw_pairs，可跨文件累计，减少内存。
  - 配对规则若不适合（如 source 不唯一），可在 DataLoader.pair_samples 中替换为 sample_id 匹配或自定义键，但保持“一次只消费一个模型样本”逻辑。
  - 若新增文件命名规则，调整 DataLoader.batch_load_model_output_dir/batch_load_annotation_dir 的过滤扩展名，或在 batch_pair_samples 中添加推断列表。

**特殊场景与风险提示**

- 字符串乱码/缺引号：部分文件存在被截断的中文或引号（如 "水平区域检�?...），对功能不影响但需谨慎编辑时修正。
- AP 无置信度：当前实现相当于单点 PR，适合“输出框集合”型评估；若模型提供 score，需在 detection 模块按 score 排序并重新计算 PR 曲线。
- Caption 指标依赖 pycocoevalcap 与 Java（Meteor），环境缺失会直接报错。
- 空样本处理：多处会跳过空 gt/pred，导致总数减少；需要严格样本计数时要审视这些跳过条件。
- API 类 (api.py) 仅为接口占位，实际逻辑需按当前 Runner/Core 实现填充。

**现有配置与示例**

- 目录配置样例：config/cfg_count.json、cfg_quyu*.json 等，指定 Windows 绝对路径和输出 JSON 名；直接 python eval_main.py --config <cfg> 可跑目录模式。
- 任务清单与指标映射：config/task_metric.json 仅列出部分映射（中文键值有缺字，需要修复后再用）。

**后续可选行动**

1. 如需正式对外文档，建议修正源码中的乱码/引号并补全 api.py 的实际实现。
2. 增加 segmentation 指标模块与 Parser/Validator 分支，完成像素级任务支持。
3. 在 README 新增 CLI 示例（单文件/目录）和依赖安装提示（pycocoevalcap/Java），并说明 AP 无置信度的含义。